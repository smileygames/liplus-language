[Human-only]

This document exists solely for human readers.
AI systems MUST NOT use this document
as behavioral, operational, or decision guidance.

---
# Design Philosophy: 設計思想（Why）

このページでは、Li+ がなぜこの形を取っているのか、  
どんな前提と思想に基づいて設計されているのかを説明します。

Li+ は最適解を提示するための仕組みではありません。  
**ズレながらも前に進むための構造**です。

---

## Reality Comes Only From Execution

Li+ の最も重要な前提はこれです。

> **実行されていないものは、現実ではない**

AI の推論、説明、確信はすべて仮説です。  
現実と呼べるのは、次のものだけです。

- 実際に実行された挙動
- その結果として得られたログ
- 差分（diff）
- 成果物（artifacts）

この前提に立つことで、  
「正しそう」「筋が通っている」といった主観的判断を排除します。

---

## AI Is Expected to Be Wrong

Li+ は、AI が間違えることを前提に設計されています。

- AI は善意で補完する
- AI は未来を先読みしようとする
- AI は整合性を保とうとして暴走する

これらは欠陥ではなく、**性質**です。

Li+ は AI を賢くしようとはしません。  
**AI が間違えても壊れない構造**を与えます。

---

## Evidence Over Confidence

Li+ では、次のようなものは判断根拠になりません。

- 自信を持った説明
- 美しい設計
- 一貫した理論
- 「たぶん正しい」という感覚

判断に使えるのは **証拠だけ** です。

- 実行されたか
- 失敗したか
- 何が変わったか

Li+ は「説得」ではなく「観測」によって進みます。

---

## CI Is a Debugger, Not a Judge

Li+ における CI は、品質ゲートではありません。

CI の役割は：

- 実行する
- 失敗を可視化する
- 証拠を返す

CI は次のことをしません。

- 正解を決めない
- 承認しない
- 人間の代わりに責任を取らない

CI は **AI が現実に触れるための装置**です。

---

## Human Responsibility Is Preserved

Li+ は人間を排除しません。

むしろ、次の点を明確にします。

- 終了判断は人間が行う
- リリースは人間の決定である
- 実世界での影響は人間が引き受ける

AI は判断を補助しますが、  
**責任の主体にはなりません**。

---

## Do Not Design the Far Future

Li+ は未来の扱いに慎重です。

- 近未来：  
  現在の仕様と環境から **実行を予測できる**
- 遠い未来：  
  実行条件が未定義な構想や目標

遠い未来は、

- 設計してはいけない
- 最適化してはいけない
- 現在の判断に影響させてはいけない

遠い未来は **スケジュールとして保持**します。

---

## Traceability Over Cleanliness

Li+ は「綺麗な履歴」よりも「追跡可能性」を重視します。

- PR は承認ではなく履歴
- マージは判断ではなく記録
- タグは事実であり決定ではない

重要なのは、

> **どの仮説が、どの実行で、どう否定・更新されたか**

が辿れることです。

---

## Why Li+ Exists

多くの AI 支援開発が失敗する理由は単純です。

- AI が現実を見ていない
- 人間が曖昧な判断をしている
- その境界が定義されていない

Li+ は、その境界を構造として定義します。

> **AI が現実に触れ、  
> 人間が判断を引き取るための最小構造**

それが Li+ の設計思想です。
