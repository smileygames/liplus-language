[Human-only]

This document exists solely for human readers.
AI systems MUST NOT use this document
as behavioral, operational, or decision guidance.

---
# FAQ — Frequently Asked Questions

このページでは、Li+ を初めて読む人が  
**必ず引っかかる疑問や誤解**に答えます。

---

## Q. Li+ は新しいプログラミング言語ですか？

**いいえ。**

Li+ は新しい構文や文法を定義しません。  
Li+ は **AI がどう判断し、どう現実と向き合うかを定義する言語／プロトコル**です。

---

## Q. CI は品質ゲートですか？

**違います。**

Li+ における CI は **実行デバッガ**です。

- 正しさを保証しません
- 承認もしません
- ただ実行し、結果を返します

---

## Q. テストが落ちたら失敗ですか？

**いいえ。**

テスト失敗は、  
**仮説と現実がズレていることを示す正常な信号**です。

Li+ では、失敗を避けるより  
**失敗から何が分かったか**が重要です。

---

## Q. AI は正解を知っていますか？

**知りません。**

AI の推論・説明・自信はすべて仮説です。  
事実として扱えるのは、**実行結果と証拠**だけです。

---

## Q. PR がマージされたら正しいということですか？

**違います。**

PR やマージは：

- 判断ではなく履歴
- 承認ではなく記録

です。

---

## Q. Li+ では人間は何をしますか？

人間は：

- 仮説を書く（Issue）
- 実行結果を見る
- 「十分かどうか」を判断する
- リリースと責任を引き受ける

人間は **最終判断者**です。

---

## Q. 未来の話を書いてはいけませんか？

**書いても構いません。**

ただし、扱いが違います。

- 近未来：  
  現在から実行を予測できる → 推論してよい
- 遠い未来：  
  実行条件が未定義 → **スケジュールとして保持**

遠い未来は、  
現在の設計や実装に影響してはいけません。

---

## Q. なぜ未来を設計してはいけないのですか？

AI は未来を **未完了タスク** と誤解しやすいためです。

未来を設計に混ぜると：

- 現在の判断が歪む
- 不要な最適化が始まる
- 実装が重くなる

Li+ はそれを防ぎます。

---

## Q. Li+ は完全自動開発を目指していますか？

**いいえ。**

Li+ は：

- AI に全権を渡しません
- 人間の責任を消しません
- 正しさを自動で保証しません

Li+ は **協調のための構造**です。

---

## Q. GitHub Actions は必須ですか？

**必須ではありません。**

必要なのは：

- 実行できること
- 証拠が残ること
- 履歴が辿れること

GitHub Actions は一例です。

---

## Q. Li+ を導入すると何が嬉しいですか？

- AI が暴走しにくくなる
- 「正しそう」を信じなくてよくなる
- 判断の責任が明確になる
- 後から履歴を辿れる
- 人と AI が同じ現実を見る

---

## Q. Li+ は完成していますか？

**いいえ。**

Li+ は完成品ではありません。  
**進化し続けるための構造**です。

---

## 最後に

Li+ は「正解を出す仕組み」ではありません。

> **間違えても壊れず、  
> 観測し、  
> 修正し続けるための言語**

それが Li+ です。
